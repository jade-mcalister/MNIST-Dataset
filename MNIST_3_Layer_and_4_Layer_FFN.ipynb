{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjsXgy1SVV8QcyZZ3E9ZEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jade-mcalister/MNIST-Dataset/blob/development/MNIST_3_Layer_and_4_Layer_FFN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We learned two linear models – linear regression and logistic regression. Compare both methods. Can we use a linear regression model to detect person’s face in an image? Describe your rationale behind it.\n",
        "\n",
        "Linear and logistic regression are both popular AI models, although serve different purposes. Linear regression works by creating and optimizing a line of best fit for the provided data points, then using this equation to predict future points. It works well for providing  floating-point outputs, such as predicting a person’s salary or a company’s predicted number of customers per year. Conversely, logistic regression generally works best with binary data, such as “yes” and “no”, or 1 and 0. It places data on a sigmoid curve, using probabilities based on each point’s corresponding features to predict the likelihood of resulting in one output or the other. Linear regression alone cannot be used for facial recongition. This is because linear regression outputs floating-point values and thus does not work for classification. Facial recognition, however, is a classification problem which should output either a \"yes\" (face matches) or a \"no\" (face does not match)."
      ],
      "metadata": {
        "id": "7tDyuzJn_c95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. In logistic regression classifier, we are fitting a s-shape curve to fit the data. We are given 10 sample points with corresponding probabilities as follows, 0.34, 0.21, 0.54, 0.45, 0.60, 0.70, 0.80, 0.95, 0.99.\n",
        "\n",
        " # (i) What are log odds? Compute log-odds values for those given data points. [Points .7]\n",
        "\n",
        "\n",
        "\n",
        "P  -> P/(1-P) -> ln(P/(1-P)):\n",
        "\n",
        "\n",
        "0.34  -> .34/(1-.34)= 0.515  -> ln(0.515) = -0.664\n",
        "\n",
        "0.21  -> .21/(1-.21)= 0.266  -> ln(0.266) = -1.324\n",
        "\n",
        "0.54  -> .54/(1-.54)= 1.174  -> ln(1.174) =  1.604\n",
        "\n",
        "0.45  -> .45/(1-.45)= 0.818  -> ln(0.818) = -0.201\n",
        "\n",
        "0.60  -> .60/(1-.60)= 1.500  -> ln(1.500) =  0.405\n",
        "\n",
        "0.70  -> .70/(1-.70)= 2.333  -> ln(2.333) =  0.847\n",
        "\n",
        "0.80  -> .80/(1-.80)= 4.000  -> ln(4.000) =  1.386\n",
        "\n",
        "0.95  -> .95/(1-.95)= 19.00  -> ln(19.00) =  2.944\n",
        "\n",
        "0.99  -> .99/(1-.99)= 99.00  -> ln(99.00) =  4.595\n",
        "\n",
        "\n",
        "# (ii) Compute log-likelihood for this given data point. [Points .8]\n",
        "\n",
        "p = (e^log(odds))/(1+e^log(odds))\n",
        "\n",
        "-0.664 -> (e^(-.664))/(1+e^(-.664)) = 0.3398\n",
        "\n",
        "-1.324 -> (e^(-1.324))/(1+e^(-1.324)) = 0.2101\n",
        "\n",
        " 1.604 -> ... = 8.326\n",
        "\n",
        "-0.201 -> ... = 0.450\n",
        "\n",
        " 0.405 -> ... = 0.599\n",
        "\n",
        " 0.847 -> ... = 0.699\n",
        "\n",
        " 1.386 -> ... = 0.799\n",
        "\n",
        " 2.944 -> ... = 0.949\n",
        "\n",
        " 4.595 -> ... = 0.989\n",
        "\n",
        "ln(.3398)+ln(.2101)+ln(8.326)+ln(0.45)+ln(0.599)+ln(0.699)+ln(0.799)+ln(0.949)+ln(0.989) = ln(0.08398)\n",
        "\n",
        "\n",
        "ln(0.08398) = -2.477"
      ],
      "metadata": {
        "id": "L2AFdFF-AJSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. We know logistic regression is a binary classifier. Can we use it for multiclass classification? Provide a detailed rationale behind your answer and include any drawback of your proposed approaches.\n",
        "\n",
        "No, I do not think it would be useful for multiclass classification. Although it can probably be done somewhat successfully using one-hot encoding or another form of variable manipulation, logistic regression is still designed to produce only binary outputs. As a result, it would not be very effective for classification problems that require more than just a 1 or 0 as an output."
      ],
      "metadata": {
        "id": "fqjZwJRJKMao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. SoftMax is a multiclass classifier, and it converts logits to probabilities. We are given logit values 3.5, 6.1, -2.9, -1.2 for 4 classes “bus”, “truck”, “car”, “van”, respectively. Compute the probability of those given logits and classify it.\n",
        "\n"
      ],
      "metadata": {
        "id": "j6tEwcGVLyq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ3V_naZ_b6Z",
        "outputId": "1d355b16-ebda-4d99-c48e-4925474b9985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels\t|Logits | Probabilities\n",
            "_____________________________\n",
            "Bus \t| 3.5\t| 0.06909\n",
            "Truck \t| 6.1\t| 0.93017\n",
            "Car \t| -2.9\t| 0.00011\n",
            "Van \t| -1.2\t| 0.00063\n",
            "\n",
            "Classification: Truck\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define labels and logits\n",
        "labels = [\"Bus\", \"Truck\", \"Car\", \"Van\"]\n",
        "logits = [3.5, 6.1, -2.9, -1.2]\n",
        "\n",
        "# Define list of probabilities\n",
        "probs = []\n",
        "normalized_probs = []\n",
        "\n",
        "\n",
        "# Calculate probabilities\n",
        "for logit in logits:\n",
        "\n",
        "    # Calculate e^logit for each value\n",
        "    exp_logit = np.e ** (logit)\n",
        "\n",
        "    # Sum the values\n",
        "    sum_exp_logits = np.sum(np.exp(logits))\n",
        "\n",
        "    # Plug into formula P = e^logit[i]/sum(e^logit)\n",
        "    p = exp_logit / sum_exp_logits\n",
        "\n",
        "    # Add to list\n",
        "    probs.append(round(p, 5))\n",
        "\n",
        "# Print output\n",
        "print(\"Labels\\t|Logits | Probabilities\")\n",
        "\n",
        "print(\"_____________________________\")\n",
        "\n",
        "for i in range(len(labels)):\n",
        "\n",
        "    print(f\"{labels[i]} \\t| {logits[i]}\\t| {probs[i]}\")\n",
        "\n",
        "# Find most likely label and classify it\n",
        "most_likely = np.argmax(probs)\n",
        "\n",
        "print(f\"\\nClassification: {labels[most_likely]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. We are designing a 2-layer feedforward neural network. Our input features are 3-dimensional. The first hidden layer has 5 neurons with sigmoid activation function. Final layer contains two neurons with Relu activation function. Assume that given inputs are x1, x2, x3 and hidden layer weights are wlij, where l ∈ {1, 2} is the layer number and i ∈ {1,2,3} is the number of inputs. j ∈ {1,2,3,4,5} indicates the number of neurons. blj indicates bias for corresponding layers and neurons. For any inconsistency with the notation given, you can modify it and mentioned the notation scheme in your answer.\n",
        "\n",
        " # (i) Draw a complete diagram of this feed forward neural network showing all individual weights, biases. [Points 1]\n",
        "\n",
        "Hand-drawn diagram will be attached as a separate pdf.\n",
        "\n",
        " # (ii) Show forward computation for this given input x1, x2, x3. Show detailed equations for each computing unit (neuron) for each layer. [Points 1.5]\n",
        "\n",
        "\n",
        "First, calculate the linear combinations and apply the sigmoid function to the values traveling between the input and hidden layers:\n",
        "\n",
        "...\n",
        "\n",
        "a11 = w111 * x1 + w121 * x2 + w131 * x3 + theta11\n",
        "\n",
        "omega1 = 1 / (1 + e^-(a11))\n",
        "\n",
        "a12 = w112 * x1 + w122 * x2 + w132 * x3 + theta12\n",
        "\n",
        "omega2 = 1 / (1 + e^-(a12))\n",
        "\n",
        "a13 = w113 * x1 + w123 * x2 + w133 * x3 + theta13\n",
        "\n",
        "omega3 = 1 / (1 + e^-(a13))\n",
        "\n",
        "a14 = w114 * x1 + w124 * x2 + w134 * x3 + theta14\n",
        "\n",
        "omega4 = 1 / (1 + e^-(a14))\n",
        "\n",
        "a115 = w115 * x1 + w125 * x2 + w135 + theta15\n",
        "\n",
        "omega5 = 1 / (1 + e^-(a15))\n",
        "\n",
        "...\n",
        "\n",
        "Next, calculate the linear combination and apply the Relu function to all values passing from the hidden layer to the output layer.\n",
        "\n",
        "...\n",
        "\n",
        "a21 = w211 * omega1 + w221 * omega2 + w231 * omega3 + w241 * omega4 + w251 * omega5 + theta21\n",
        "\n",
        "r1 = max(0, a21)\n",
        "\n",
        "a22 = w212 * omega1 + w222 * omega2 + w232 * omega3 + w242 * omega4 + w252 * omega5 + theta22\n",
        "\n",
        "r2 = max(0, a22)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HsGvFRppVyIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. In this homework, we are going to implement a neural network for the handwritten digit classification problem with the MNIST data. Please use the MNIST data that includes 100 images on each label of 0 – 9. Dataset is given as a CSV file. Each row represents an image with the given class label in the first column.You should implement a neural network (NN) and first create your training and testing with a random split of 80% training and 20% testing data. Compute accuracy for the test dataset.Compare performance 3-layer NN, and 4-layer NN. For classification tasks using the SoftMaxclassifier at the end of your NN. You can design the network by yourself."
      ],
      "metadata": {
        "id": "oIiotsm4ndbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define the transformation to convert images to PyTorch tensors and normalize the pixel values\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load MNIST Dataset\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training data into training (80%) and testing (20%) sets\n",
        "train_size = int(0.8 * len(mnist_train))\n",
        "val_size = len(mnist_train) - train_size\n",
        "mnist_train, mnist_val = random_split(mnist_train, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and test sets\n",
        "train_loader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(mnist_val, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Create three-layer model\n",
        "\n",
        "class ThreeLayerNN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Initialize layers and relu function\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input\n",
        "        x = x.view(-1, 28*28)\n",
        "\n",
        "        # Apply forward pass\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_3_layer = ThreeLayerNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_3_layer.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create four-layer model\n",
        "class FourLayerNN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Initialize layers and relu function\n",
        "        super(FourLayerNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input\n",
        "        x = x.view(-1, 28*28)\n",
        "\n",
        "        # Apply forward pass\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_4_layer = FourLayerNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_4_layer = optim.Adam(model_4_layer.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "# Train the models\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=150):\n",
        "    for epoch in range(epochs):\n",
        "        #\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_accuracy = 100. * correct / total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = output.max(1)\n",
        "                total += target.size(0)\n",
        "                correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_accuracy = 100. * correct / total\n",
        "\n",
        "        if epoch==0 or (1+epoch) % 10 == 0 :\n",
        "          print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the 3-layer model\n",
        "trained_model_3_layer = train_model(model_3_layer, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "# Train the 4-layer model\n",
        "trained_model_4_layer = train_model(model_4_layer, train_loader, val_loader, criterion, optimizer_4_layer)\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "    return test_accuracy\n",
        "\n",
        "# Evaluate the 3-layer model\n",
        "test_accuracy_3_layer = evaluate_model(trained_model_3_layer, test_loader, criterion)\n",
        "\n",
        "# Evaluate the 4-layer model\n",
        "test_accuracy_4_layer = evaluate_model(trained_model_4_layer, test_loader, criterion)\n",
        "\n",
        "\n",
        "print(f'3-layer NN Test Accuracy: {test_accuracy_3_layer:.2f}%')\n",
        "print(f'4-layer NN Test Accuracy: {test_accuracy_4_layer:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-3KAIp9HmrN",
        "outputId": "e2d3bed0-2e95-4455-c443-dd531bb904f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 0.0509, Train Accuracy: 84.46%, Val Loss: 0.0490, Val Accuracy: 89.53%\n",
            "Epoch: 10, Train Loss: 0.0469, Train Accuracy: 95.95%, Val Loss: 0.0472, Val Accuracy: 95.03%\n",
            "Epoch: 20, Train Loss: 0.0467, Train Accuracy: 96.77%, Val Loss: 0.0475, Val Accuracy: 94.00%\n",
            "Epoch: 30, Train Loss: 0.0466, Train Accuracy: 96.88%, Val Loss: 0.0468, Val Accuracy: 96.20%\n",
            "Epoch: 40, Train Loss: 0.0466, Train Accuracy: 97.16%, Val Loss: 0.0471, Val Accuracy: 95.40%\n",
            "Epoch: 50, Train Loss: 0.0466, Train Accuracy: 97.01%, Val Loss: 0.0467, Val Accuracy: 96.67%\n",
            "Epoch: 60, Train Loss: 0.0466, Train Accuracy: 96.92%, Val Loss: 0.0469, Val Accuracy: 96.12%\n",
            "Epoch: 70, Train Loss: 0.0467, Train Accuracy: 96.81%, Val Loss: 0.0468, Val Accuracy: 96.51%\n",
            "Epoch: 80, Train Loss: 0.0466, Train Accuracy: 97.12%, Val Loss: 0.0468, Val Accuracy: 96.34%\n",
            "Epoch: 90, Train Loss: 0.0466, Train Accuracy: 97.09%, Val Loss: 0.0467, Val Accuracy: 96.60%\n",
            "Epoch: 100, Train Loss: 0.0466, Train Accuracy: 97.03%, Val Loss: 0.0473, Val Accuracy: 94.83%\n",
            "Epoch: 110, Train Loss: 0.0465, Train Accuracy: 97.35%, Val Loss: 0.0467, Val Accuracy: 96.64%\n",
            "Epoch: 120, Train Loss: 0.0466, Train Accuracy: 97.14%, Val Loss: 0.0468, Val Accuracy: 96.41%\n",
            "Epoch: 130, Train Loss: 0.0466, Train Accuracy: 96.90%, Val Loss: 0.0468, Val Accuracy: 96.49%\n",
            "Epoch: 140, Train Loss: 0.0466, Train Accuracy: 97.11%, Val Loss: 0.0468, Val Accuracy: 96.23%\n",
            "Epoch: 150, Train Loss: 0.0466, Train Accuracy: 97.07%, Val Loss: 0.0469, Val Accuracy: 96.18%\n",
            "Epoch: 1, Train Loss: 0.0530, Train Accuracy: 77.40%, Val Loss: 0.0495, Val Accuracy: 87.89%\n",
            "Epoch: 10, Train Loss: 0.0472, Train Accuracy: 95.06%, Val Loss: 0.0471, Val Accuracy: 95.28%\n",
            "Epoch: 20, Train Loss: 0.0469, Train Accuracy: 95.92%, Val Loss: 0.0470, Val Accuracy: 95.75%\n",
            "Epoch: 30, Train Loss: 0.0470, Train Accuracy: 95.84%, Val Loss: 0.0469, Val Accuracy: 95.97%\n",
            "Epoch: 40, Train Loss: 0.0469, Train Accuracy: 96.03%, Val Loss: 0.0473, Val Accuracy: 94.61%\n",
            "Epoch: 50, Train Loss: 0.0470, Train Accuracy: 95.85%, Val Loss: 0.0474, Val Accuracy: 94.48%\n",
            "Epoch: 60, Train Loss: 0.0470, Train Accuracy: 95.63%, Val Loss: 0.0475, Val Accuracy: 94.20%\n",
            "Epoch: 70, Train Loss: 0.0472, Train Accuracy: 94.96%, Val Loss: 0.0472, Val Accuracy: 95.13%\n",
            "Epoch: 80, Train Loss: 0.0472, Train Accuracy: 95.11%, Val Loss: 0.0475, Val Accuracy: 94.17%\n",
            "Epoch: 90, Train Loss: 0.0474, Train Accuracy: 94.49%, Val Loss: 0.0474, Val Accuracy: 94.53%\n",
            "Epoch: 100, Train Loss: 0.0471, Train Accuracy: 95.36%, Val Loss: 0.0471, Val Accuracy: 95.27%\n",
            "Epoch: 110, Train Loss: 0.0475, Train Accuracy: 94.19%, Val Loss: 0.0473, Val Accuracy: 94.60%\n",
            "Epoch: 120, Train Loss: 0.0471, Train Accuracy: 95.28%, Val Loss: 0.0474, Val Accuracy: 94.48%\n",
            "Epoch: 130, Train Loss: 0.0472, Train Accuracy: 95.21%, Val Loss: 0.0472, Val Accuracy: 95.03%\n",
            "Epoch: 140, Train Loss: 0.0474, Train Accuracy: 94.47%, Val Loss: 0.0473, Val Accuracy: 94.67%\n",
            "Epoch: 150, Train Loss: 0.0476, Train Accuracy: 93.73%, Val Loss: 0.0491, Val Accuracy: 89.03%\n",
            "Test Loss: 0.0468, Test Accuracy: 96.59%\n",
            "Test Loss: 0.0490, Test Accuracy: 89.66%\n",
            "3-layer NN Test Accuracy: 96.59%\n",
            "4-layer NN Test Accuracy: 89.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Overall, the two models relatively close. However, the three-layer model actually worked slightly better than the four-layer model. This might be because the simplicity of the data made a fourth layer unnecessary, so having the extra layer might have just led to more error due to overfitting."
      ],
      "metadata": {
        "id": "ue3Trn7qSbiW"
      }
    }
  ]
}